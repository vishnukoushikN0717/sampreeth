{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceeed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, models\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class NOMA_PPO:\n",
    "    def __init__(self, \n",
    "                 num_users=10,\n",
    "                 max_buffer_size=5,\n",
    "                 discount_factor=0.99,\n",
    "                 gae_lambda=0.95,\n",
    "                 ppo_epsilon=0.2,\n",
    "                 actor_learning_rate=0.0003,\n",
    "                 critic_learning_rate=0.001,\n",
    "                 entropy_coef=0.01,\n",
    "                 batch_size=64,\n",
    "                 channel_threshold=0.1,\n",
    "                 tau_threshold=5,\n",
    "                 max_noma_users=4):\n",
    "        \"\"\"\n",
    "        Initialize the NOMA_PPO agent.\n",
    "        \n",
    "        Args:\n",
    "            num_users: Number of users in the system\n",
    "            max_buffer_size: Maximum buffer size for each user\n",
    "            discount_factor: Discount factor for future rewards\n",
    "            gae_lambda: Lambda parameter for GAE\n",
    "            ppo_epsilon: Clipping parameter for PPO\n",
    "            actor_learning_rate: Learning rate for the actor network\n",
    "            critic_learning_rate: Learning rate for the critic network\n",
    "            entropy_coef: Coefficient for entropy term in loss function\n",
    "            batch_size: Batch size for training\n",
    "            channel_threshold: Threshold for channel quality\n",
    "            tau_threshold: Threshold for channel age\n",
    "            max_noma_users: Maximum number of users that can be scheduled simultaneously (B)\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        self.gamma = discount_factor\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_ratio = ppo_epsilon\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.batch_size = batch_size\n",
    "        self.eta_threshold = channel_threshold\n",
    "        self.tau_threshold = tau_threshold\n",
    "        self.max_noma_users = max_noma_users  # B value in the paper\n",
    "        \n",
    "        # Input size: buffer info + timing info + channel info + last reward\n",
    "        self.input_size = 5 * num_users + 1\n",
    "        \n",
    "        # Build the actor and critic networks\n",
    "        self.actor = self._build_actor_network()\n",
    "        self.critic = self._build_critic_network()\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_learning_rate)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_learning_rate)\n",
    "        \n",
    "        # Experience buffer\n",
    "        self.buffer = []\n",
    "        \n",
    "    def _build_actor_network(self):\n",
    "        \"\"\"Builds the actor network with branching architecture as described in the paper\"\"\"\n",
    "        inputs = layers.Input(shape=(self.input_size,))\n",
    "        \n",
    "        # Shared layers\n",
    "        x = layers.Dense(128, activation='relu')(inputs)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        \n",
    "        # Branches for each user (one output per user)\n",
    "        output_layers = []\n",
    "        for _ in range(self.num_users):\n",
    "            branch = layers.Dense(32, activation='relu')(x)\n",
    "            # Output activation probability for each user\n",
    "            branch_output = layers.Dense(1, activation='sigmoid')(branch)\n",
    "            output_layers.append(branch_output)\n",
    "        \n",
    "        # Combine all branch outputs\n",
    "        outputs = layers.Concatenate()(output_layers)\n",
    "        \n",
    "        return models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    def _build_critic_network(self):\n",
    "        \"\"\"Builds the critic network to estimate value function\"\"\"\n",
    "        inputs = layers.Input(shape=(self.input_size,))\n",
    "        x = layers.Dense(128, activation='relu')(inputs)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        value = layers.Dense(1, activation=None)(x)\n",
    "        \n",
    "        return models.Model(inputs=inputs, outputs=value)\n",
    "    \n",
    "    def get_edf_prior(self, buffer_info):\n",
    "        \"\"\"\n",
    "        Earliest Deadline First prior\n",
    "        Returns scheduling priority based on deadlines\n",
    "        \"\"\"\n",
    "        # Extract head-of-line delays from buffer_info\n",
    "        hol_delays = buffer_info[:self.num_users]  # Assuming first values are head-of-line delays\n",
    "        \n",
    "        # Sort users by increasing deadline (smaller deadline = higher priority)\n",
    "        sorted_indices = np.argsort(hol_delays)\n",
    "        \n",
    "        # Initialize prior with zeros\n",
    "        prior = np.zeros(self.num_users)\n",
    "        \n",
    "        # Set priority 1 for the B users with the smallest deadline\n",
    "        for i in range(min(self.max_noma_users, len(sorted_indices))):\n",
    "            if hol_delays[sorted_indices[i]] > 0:  # Only if there's a packet\n",
    "                prior[sorted_indices[i]] = 1.0\n",
    "                \n",
    "        return prior\n",
    "    \n",
    "    def get_channel_prior(self, channel_info, channel_age):\n",
    "        \"\"\"\n",
    "        Channel quality prior\n",
    "        Returns priority based on channel conditions\n",
    "        \"\"\"\n",
    "        # Extract channel quality and age\n",
    "        channel_gains = channel_info[:self.num_users]\n",
    "        ages = channel_age[:self.num_users]\n",
    "        \n",
    "        # Initialize prior with ones\n",
    "        prior = np.ones(self.num_users)\n",
    "        \n",
    "        # Set to 0 for poor channels or outdated channel information\n",
    "        for i in range(self.num_users):\n",
    "            if channel_gains[i] <= self.eta_threshold and ages[i] <= self.tau_threshold:\n",
    "                prior[i] = 0.0\n",
    "                \n",
    "        return prior\n",
    "    \n",
    "    def combine_priors(self, edf_prior, channel_prior):\n",
    "        \"\"\"Combine EDF and channel priors element-wise\"\"\"\n",
    "        return edf_prior * channel_prior\n",
    "    \n",
    "    def get_policy_with_prior(self, agent_state):\n",
    "        \"\"\"\n",
    "        Apply Bayesian policy using prior knowledge\n",
    "        q(a|A; θπ) ∝ π(a|A; θπ) ⊙ f(a;A)\n",
    "        \"\"\"\n",
    "        # Parse state components\n",
    "        buffer_info = agent_state[:self.num_users * 2]  # Buffer info\n",
    "        timing_info = agent_state[self.num_users * 2:self.num_users * 4]  # τp, τa, τs\n",
    "        channel_info = agent_state[self.num_users * 4:self.num_users * 5]  # η\n",
    "        \n",
    "        # Get policy from neural network\n",
    "        raw_policy = self.actor(np.array([agent_state])).numpy()[0]\n",
    "        \n",
    "        # Get EDF prior\n",
    "        edf_prior = self.get_edf_prior(buffer_info)\n",
    "        \n",
    "        # Get channel prior\n",
    "        channel_prior = self.get_channel_prior(channel_info, timing_info[:self.num_users])\n",
    "        \n",
    "        # Combine priors\n",
    "        combined_prior = self.combine_priors(edf_prior, channel_prior)\n",
    "        \n",
    "        # Apply Bayesian update (element-wise multiplication)\n",
    "        posterior_policy = raw_policy * combined_prior\n",
    "        \n",
    "        # Normalize if needed\n",
    "        if np.sum(posterior_policy) > 0:\n",
    "            posterior_policy = posterior_policy / np.sum(posterior_policy)\n",
    "            \n",
    "        return posterior_policy\n",
    "    \n",
    "    def choose_action(self, agent_state, deterministic=False):\n",
    "        \"\"\"Choose action based on policy with prior knowledge\"\"\"\n",
    "        policy = self.get_policy_with_prior(agent_state)\n",
    "        \n",
    "        if deterministic:\n",
    "            # Select users with highest probability up to max_noma_users\n",
    "            sorted_indices = np.argsort(-policy)\n",
    "            action = np.zeros(self.num_users)\n",
    "            count = 0\n",
    "            for idx in sorted_indices:\n",
    "                if policy[idx] > 0.5 and count < self.max_noma_users:\n",
    "                    action[idx] = 1\n",
    "                    count += 1\n",
    "        else:\n",
    "            # Sample action for each user independently\n",
    "            action = np.zeros(self.num_users)\n",
    "            for i in range(self.num_users):\n",
    "                if random.random() < policy[i]:\n",
    "                    action[i] = 1\n",
    "            \n",
    "            # Ensure we don't exceed max_noma_users\n",
    "            if np.sum(action) > self.max_noma_users:\n",
    "                # If too many users selected, keep only the top max_noma_users\n",
    "                indices = np.argsort(-policy)\n",
    "                action = np.zeros(self.num_users)\n",
    "                for i in range(self.max_noma_users):\n",
    "                    action[indices[i]] = 1\n",
    "                    \n",
    "        return action\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def compute_gae(self, rewards, values, next_values, dones):\n",
    "        \"\"\"Compute Generalized Advantage Estimation\"\"\"\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        last_advantage = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            # If episode is done, use reward as terminal value\n",
    "            if dones[t]:\n",
    "                delta = rewards[t] - values[t]\n",
    "            else:\n",
    "                delta = rewards[t] + self.gamma * next_values[t] - values[t]\n",
    "            \n",
    "            # Recursive update of advantage\n",
    "            advantages[t] = delta + self.gamma * self.gae_lambda * last_advantage * (1 - dones[t])\n",
    "            last_advantage = advantages[t]\n",
    "            \n",
    "        # Returns are advantage + value\n",
    "        returns = advantages + values\n",
    "        \n",
    "        return returns, advantages\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using PPO\"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch from buffer\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones)\n",
    "        \n",
    "        # Get values for states and next states\n",
    "        values = self.critic(states).numpy().flatten()\n",
    "        next_values = self.critic(next_states).numpy().flatten()\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns, advantages = self.compute_gae(rewards, values, next_values, dones)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
    "        \n",
    "        # Get old action probabilities\n",
    "        old_probs = np.zeros((self.batch_size, self.num_users))\n",
    "        for i in range(self.batch_size):\n",
    "            policy = self.get_policy_with_prior(states[i])\n",
    "            old_probs[i] = policy\n",
    "        \n",
    "        # PPO training loop\n",
    "        for _ in range(5):  # Multiple epochs of training\n",
    "            indices = np.arange(self.batch_size)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start in range(0, self.batch_size, 32):\n",
    "                end = start + 32\n",
    "                if end <= self.batch_size:\n",
    "                    idx = indices[start:end]\n",
    "                    \n",
    "                    with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "                        # Get current policy\n",
    "                        current_policy = self.actor(states[idx])\n",
    "                        \n",
    "                        # Apply prior\n",
    "                        modified_policy = tf.zeros_like(current_policy)\n",
    "                        for i, idx_val in enumerate(idx):\n",
    "                            prior = tf.convert_to_tensor(self.get_channel_prior(\n",
    "                                states[idx_val][self.num_users*4:self.num_users*5],\n",
    "                                states[idx_val][self.num_users*2:self.num_users*3]\n",
    "                            ) * self.get_edf_prior(states[idx_val][:self.num_users*2]))\n",
    "                            modified_policy[i] = current_policy[i] * prior\n",
    "                        \n",
    "                        # Compute ratio\n",
    "                        ratio = tf.reduce_prod(\n",
    "                            actions[idx] * modified_policy + (1 - actions[idx]) * (1 - modified_policy), axis=1\n",
    "                        ) / tf.reduce_prod(\n",
    "                            actions[idx] * old_probs[idx] + (1 - actions[idx]) * (1 - old_probs[idx]), axis=1\n",
    "                        )\n",
    "                        \n",
    "                        # Compute surrogate losses\n",
    "                        surrogate1 = ratio * advantages[idx]\n",
    "                        surrogate2 = tf.clip_by_value(ratio, 1-self.clip_ratio, 1+self.clip_ratio) * advantages[idx]\n",
    "                        \n",
    "                        # Actor loss\n",
    "                        actor_loss = -tf.reduce_mean(tf.minimum(surrogate1, surrogate2))\n",
    "                        \n",
    "                        # Add entropy bonus\n",
    "                        entropy = -tf.reduce_mean(\n",
    "                            modified_policy * tf.math.log(modified_policy + 1e-10) + \n",
    "                            (1 - modified_policy) * tf.math.log(1 - modified_policy + 1e-10)\n",
    "                        )\n",
    "                        actor_loss = actor_loss - self.entropy_coef * entropy\n",
    "                        \n",
    "                        # Critic loss (MSE)\n",
    "                        value_preds = self.critic(states[idx])\n",
    "                        critic_loss = tf.reduce_mean(tf.square(returns[idx] - value_preds))\n",
    "                    \n",
    "                    # Compute gradients and apply updates\n",
    "                    actor_grads = actor_tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "                    critic_grads = critic_tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "                    \n",
    "                    self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "                    self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        \n",
    "        # Clear buffer after training\n",
    "        self.buffer = []\n",
    "\n",
    "# Simplified example of usage\n",
    "def preprocess_state(buffer_status, timing_info, channel_info, last_reward):\n",
    "    \"\"\"\n",
    "    Preprocess state information into the format expected by the agent.\n",
    "    \n",
    "    Args:\n",
    "        buffer_status: Buffer information (B)\n",
    "        timing_info: Timing information (τp, τa, τs)\n",
    "        channel_info: Channel gains (η)\n",
    "        last_reward: Last received reward r(t-1)\n",
    "        \n",
    "    Returns:\n",
    "        Concatenated and preprocessed state vector\n",
    "    \"\"\"\n",
    "    # Concatenate all information as described in the paper\n",
    "    return np.concatenate([buffer_status, timing_info, channel_info, [last_reward]])\n",
    "\n",
    "def example_usage():\n",
    "    # Example parameters\n",
    "    num_users = 10\n",
    "    max_buffer_size = 5\n",
    "    \n",
    "    # Initialize the NOMA-PPO agent\n",
    "    agent = NOMA_PPO(\n",
    "        num_users=num_users,\n",
    "        max_buffer_size=max_buffer_size,\n",
    "        discount_factor=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ppo_epsilon=0.2,\n",
    "        actor_learning_rate=0.0003,\n",
    "        critic_learning_rate=0.001,\n",
    "        entropy_coef=0.01,\n",
    "        batch_size=64,\n",
    "        channel_threshold=0.1,\n",
    "        tau_threshold=5,\n",
    "        max_noma_users=4\n",
    "    )\n",
    "    \n",
    "    # Simulated environment interaction\n",
    "    for episode in range(10):\n",
    "        # Example state (would come from environment in real implementation)\n",
    "        buffer_status = np.random.randint(0, 2, size=num_users*2)  # Buffer info\n",
    "        timing_info = np.random.rand(num_users*2)  # Timing info\n",
    "        channel_info = np.random.rand(num_users)  # Channel gains\n",
    "        last_reward = 0\n",
    "        \n",
    "        state = preprocess_state(buffer_status, timing_info, channel_info, last_reward)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Choose action based on current state\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # Execute action in environment (simplified)\n",
    "            # In real implementation, this would interact with a simulator\n",
    "            reward = np.sum(action) * 0.5  # Simplified reward\n",
    "            \n",
    "            # Get next state (simplified)\n",
    "            next_buffer_status = np.random.randint(0, 2, size=num_users*2)\n",
    "            next_timing_info = np.random.rand(num_users*2)\n",
    "            next_channel_info = np.random.rand(num_users)\n",
    "            next_state = preprocess_state(next_buffer_status, next_timing_info, next_channel_info, reward)\n",
    "            \n",
    "            # Store experience\n",
    "            agent.store_experience(state, action, reward, next_state, False)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Check if episode is done (simplified)\n",
    "            done = np.random.random() < 0.05  # 5% chance of episode ending\n",
    "        \n",
    "        # Train the agent\n",
    "        agent.train()\n",
    "        \n",
    "        print(f\"Episode {episode+1}, Total Reward: {episode_reward}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda39da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "class NOMA_PPO:\n",
    "    # [All existing code remains the same]\n",
    "    \n",
    "    # Add this method to collect metrics\n",
    "    def get_metrics(self):\n",
    "        \"\"\"Return current metrics for visualization\"\"\"\n",
    "        return {\n",
    "            \"buffer_size\": len(self.buffer),\n",
    "            \"actor_loss\": getattr(self, \"last_actor_loss\", 0),\n",
    "            \"critic_loss\": getattr(self, \"last_critic_loss\", 0),\n",
    "            \"entropy\": getattr(self, \"last_entropy\", 0),\n",
    "        }\n",
    "    \n",
    "    # Modify the train method to store metrics\n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using PPO\"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return {}\n",
    "        \n",
    "        # [Keep the existing code here]\n",
    "        \n",
    "        # Track losses and metrics\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        entropy_values = []\n",
    "        ratios = []\n",
    "        \n",
    "        # PPO training loop\n",
    "        for epoch in range(5):  # Multiple epochs of training\n",
    "            indices = np.arange(self.batch_size)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start in range(0, self.batch_size, 32):\n",
    "                end = start + 32\n",
    "                if end <= self.batch_size:\n",
    "                    idx = indices[start:end]\n",
    "                    \n",
    "                    with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "                        # [Keep the existing code here]\n",
    "                        \n",
    "                        # Store metrics\n",
    "                        actor_losses.append(actor_loss.numpy())\n",
    "                        critic_losses.append(critic_loss.numpy())\n",
    "                        entropy_values.append(entropy.numpy())\n",
    "                        ratios.append(tf.reduce_mean(ratio).numpy())\n",
    "                    \n",
    "                    # [Keep the existing code here]\n",
    "        \n",
    "        # Store metrics for visualization\n",
    "        self.last_actor_loss = np.mean(actor_losses)\n",
    "        self.last_critic_loss = np.mean(critic_losses)\n",
    "        self.last_entropy = np.mean(entropy_values)\n",
    "        self.last_ratio = np.mean(ratios)\n",
    "        \n",
    "        # Clear buffer after training\n",
    "        self.buffer = []\n",
    "        \n",
    "        # Return metrics for visualization\n",
    "        return {\n",
    "            \"actor_loss\": self.last_actor_loss,\n",
    "            \"critic_loss\": self.last_critic_loss,\n",
    "            \"entropy\": self.last_entropy,\n",
    "            \"ratio\": self.last_ratio\n",
    "        }\n",
    "\n",
    "# Modify the example usage to include visualization\n",
    "def improved_example_usage():\n",
    "    # Example parameters\n",
    "    num_users = 10\n",
    "    max_buffer_size = 5\n",
    "    num_episodes = 50  # Increased for better visualization\n",
    "    \n",
    "    # Initialize the NOMA-PPO agent\n",
    "    agent = NOMA_PPO(\n",
    "        num_users=num_users,\n",
    "        max_buffer_size=max_buffer_size,\n",
    "        discount_factor=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ppo_epsilon=0.2,\n",
    "        actor_learning_rate=0.0003,\n",
    "        critic_learning_rate=0.001,\n",
    "        entropy_coef=0.01,\n",
    "        batch_size=64,\n",
    "        channel_threshold=0.1,\n",
    "        tau_threshold=5,\n",
    "        max_noma_users=4\n",
    "    )\n",
    "    \n",
    "    # Metrics for plotting\n",
    "    rewards_history = []\n",
    "    avg_rewards_history = []\n",
    "    actor_loss_history = []\n",
    "    critic_loss_history = []\n",
    "    entropy_history = []\n",
    "    scheduled_users_history = []\n",
    "    \n",
    "    # Simulated environment interaction\n",
    "    for episode in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        episode_scheduled_users = []\n",
    "        \n",
    "        # Example state (would come from environment in real implementation)\n",
    "        buffer_status = np.random.randint(0, 2, size=num_users*2).astype(np.float32)  # Buffer info\n",
    "        timing_info = np.random.rand(num_users*2).astype(np.float32)  # Timing info\n",
    "        channel_info = np.random.rand(num_users).astype(np.float32)  # Channel gains\n",
    "        last_reward = 0.0\n",
    "        \n",
    "        state = preprocess_state(buffer_status, timing_info, channel_info, last_reward)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        step = 0\n",
    "        max_steps = 20  # Limit steps per episode\n",
    "        \n",
    "        while not done and step < max_steps:\n",
    "            # Choose action based on current state\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # Track number of scheduled users\n",
    "            num_scheduled = np.sum(action)\n",
    "            episode_scheduled_users.append(num_scheduled)\n",
    "            \n",
    "            # Execute action in environment (simplified)\n",
    "            # In real implementation, this would interact with a simulator\n",
    "            reward = np.sum(action) * 0.5  # Simplified reward\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            # Add some randomness to model varying channel conditions\n",
    "            channel_quality = np.random.rand()\n",
    "            if channel_quality < 0.2:  # 20% chance of bad channel\n",
    "                reward *= 0.5  # Reduced reward due to poor channel\n",
    "            \n",
    "            # Get next state (simplified)\n",
    "            next_buffer_status = np.random.randint(0, 2, size=num_users*2).astype(np.float32)\n",
    "            next_timing_info = np.random.rand(num_users*2).astype(np.float32)\n",
    "            next_channel_info = np.random.rand(num_users).astype(np.float32)\n",
    "            next_state = preprocess_state(next_buffer_status, next_timing_info, next_channel_info, reward)\n",
    "            \n",
    "            # Store experience\n",
    "            agent.store_experience(state, action, reward, next_state, False)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Check if episode is done (simplified)\n",
    "            done = np.random.random() < 0.05  # 5% chance of episode ending\n",
    "            step += 1\n",
    "        \n",
    "        # Store episode reward\n",
    "        rewards_history.append(episode_reward)\n",
    "        \n",
    "        # Store average scheduled users\n",
    "        scheduled_users_history.append(np.mean(episode_scheduled_users))\n",
    "        \n",
    "        # Calculate moving average\n",
    "        if len(rewards_history) >= 10:\n",
    "            avg_reward = np.mean(rewards_history[-10:])\n",
    "        else:\n",
    "            avg_reward = np.mean(rewards_history)\n",
    "        avg_rewards_history.append(avg_reward)\n",
    "        \n",
    "        # Train the agent if we have enough samples\n",
    "        if len(agent.buffer) >= agent.batch_size:\n",
    "            metrics = agent.train()\n",
    "            \n",
    "            # Store training metrics\n",
    "            if metrics:\n",
    "                actor_loss_history.append(metrics[\"actor_loss\"])\n",
    "                critic_loss_history.append(metrics[\"critic_loss\"])\n",
    "                entropy_history.append(metrics[\"entropy\"])\n",
    "        \n",
    "        print(f\"Episode {episode+1}, Reward: {episode_reward:.2f}, Avg Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    plot_training_results(\n",
    "        rewards_history, \n",
    "        avg_rewards_history, \n",
    "        actor_loss_history, \n",
    "        critic_loss_history, \n",
    "        entropy_history,\n",
    "        scheduled_users_history\n",
    "    )\n",
    "\n",
    "def plot_training_results(rewards, avg_rewards, actor_losses, critic_losses, entropies, scheduled_users):\n",
    "    \"\"\"Create various plots to visualize the agent's performance\"\"\"\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Plot 1: Episode Rewards\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(rewards, label='Episode Reward')\n",
    "    plt.plot(avg_rewards, label='Moving Average (10 episodes)', linewidth=2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 2: Actor Loss\n",
    "    plt.subplot(3, 2, 2)\n",
    "    if actor_losses:\n",
    "        plt.plot(actor_losses)\n",
    "        plt.xlabel('Training Update')\n",
    "        plt.ylabel('Actor Loss')\n",
    "        plt.title('Actor Network Loss')\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No training data yet', ha='center', va='center')\n",
    "        plt.title('Actor Network Loss (No Data)')\n",
    "    \n",
    "    # Plot 3: Critic Loss\n",
    "    plt.subplot(3, 2, 3)\n",
    "    if critic_losses:\n",
    "        plt.plot(critic_losses)\n",
    "        plt.xlabel('Training Update')\n",
    "        plt.ylabel('Critic Loss')\n",
    "        plt.title('Critic Network Loss')\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No training data yet', ha='center', va='center')\n",
    "        plt.title('Critic Network Loss (No Data)')\n",
    "    \n",
    "    # Plot 4: Entropy\n",
    "    plt.subplot(3, 2, 4)\n",
    "    if entropies:\n",
    "        plt.plot(entropies)\n",
    "        plt.xlabel('Training Update')\n",
    "        plt.ylabel('Entropy')\n",
    "        plt.title('Policy Entropy')\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No training data yet', ha='center', va='center')\n",
    "        plt.title('Policy Entropy (No Data)')\n",
    "    \n",
    "    # Plot 5: Average Number of Scheduled Users\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(scheduled_users)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Avg. Users Scheduled')\n",
    "    plt.title('Average Number of Scheduled Users per Episode')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 6: Distribution of Scheduled Users\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.hist(scheduled_users, bins=range(0, 5), alpha=0.7, rwidth=0.8)\n",
    "    plt.xlabel('Number of Users Scheduled')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Scheduled Users')\n",
    "    plt.xticks(range(0, 5))\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('noma_ppo_results.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    improved_example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "class NOMA_PPO:\n",
    "    def __init__(self, \n",
    "                 num_users=10,\n",
    "                 max_buffer_size=5,\n",
    "                 discount_factor=0.99,\n",
    "                 gae_lambda=0.95,\n",
    "                 ppo_epsilon=0.2,\n",
    "                 actor_learning_rate=0.0003,\n",
    "                 critic_learning_rate=0.001,\n",
    "                 entropy_coef=0.01,\n",
    "                 batch_size=64,\n",
    "                 channel_threshold=0.1,\n",
    "                 tau_threshold=5,\n",
    "                 max_noma_users=4):\n",
    "        \"\"\"\n",
    "        Initialize the NOMA_PPO agent.\n",
    "        \n",
    "        Args:\n",
    "            num_users: Number of users in the system\n",
    "            max_buffer_size: Maximum buffer size for each user\n",
    "            discount_factor: Discount factor for future rewards\n",
    "            gae_lambda: Lambda parameter for GAE\n",
    "            ppo_epsilon: Clipping parameter for PPO\n",
    "            actor_learning_rate: Learning rate for the actor network\n",
    "            critic_learning_rate: Learning rate for the critic network\n",
    "            entropy_coef: Coefficient for entropy term in loss function\n",
    "            batch_size: Batch size for training\n",
    "            channel_threshold: Threshold for channel quality\n",
    "            tau_threshold: Threshold for channel age\n",
    "            max_noma_users: Maximum number of users that can be scheduled simultaneously (B)\n",
    "        \"\"\"\n",
    "        self.num_users = num_users\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        self.gamma = discount_factor\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_ratio = ppo_epsilon\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.batch_size = batch_size\n",
    "        self.eta_threshold = channel_threshold\n",
    "        self.tau_threshold = tau_threshold\n",
    "        self.max_noma_users = max_noma_users  # B value in the paper\n",
    "        \n",
    "        # Input size: buffer info + timing info + channel info + last reward\n",
    "        self.input_size = 5 * num_users + 1\n",
    "        \n",
    "        # Build the actor and critic networks\n",
    "        self.actor = self._build_actor_network()\n",
    "        self.critic = self._build_critic_network()\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_learning_rate)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_learning_rate)\n",
    "        \n",
    "        # Experience buffer\n",
    "        self.buffer = []\n",
    "        \n",
    "        # For tracking metrics\n",
    "        self.last_actor_loss = 0\n",
    "        self.last_critic_loss = 0\n",
    "        self.last_entropy = 0\n",
    "        self.last_ratio = 0\n",
    "        \n",
    "    def _build_actor_network(self):\n",
    "        \"\"\"Builds the actor network with branching architecture as described in the paper\"\"\"\n",
    "        inputs = layers.Input(shape=(self.input_size,))\n",
    "        \n",
    "        # Shared layers\n",
    "        x = layers.Dense(128, activation='relu')(inputs)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        \n",
    "        # Branches for each user (one output per user)\n",
    "        output_layers = []\n",
    "        for _ in range(self.num_users):\n",
    "            branch = layers.Dense(32, activation='relu')(x)\n",
    "            # Output activation probability for each user\n",
    "            branch_output = layers.Dense(1, activation='sigmoid')(branch)\n",
    "            output_layers.append(branch_output)\n",
    "        \n",
    "        # Combine all branch outputs\n",
    "        outputs = layers.Concatenate()(output_layers)\n",
    "        \n",
    "        return models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    def _build_critic_network(self):\n",
    "        \"\"\"Builds the critic network to estimate value function\"\"\"\n",
    "        inputs = layers.Input(shape=(self.input_size,))\n",
    "        x = layers.Dense(128, activation='relu')(inputs)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        value = layers.Dense(1, activation=None)(x)\n",
    "        \n",
    "        return models.Model(inputs=inputs, outputs=value)\n",
    "    \n",
    "    def get_edf_prior(self, buffer_info):\n",
    "        \"\"\"\n",
    "        Earliest Deadline First prior\n",
    "        Returns scheduling priority based on deadlines\n",
    "        \"\"\"\n",
    "        # Extract head-of-line delays from buffer_info\n",
    "        hol_delays = buffer_info[:self.num_users]  # Assuming first values are head-of-line delays\n",
    "        \n",
    "        # Sort users by increasing deadline (smaller deadline = higher priority)\n",
    "        sorted_indices = np.argsort(hol_delays)\n",
    "        \n",
    "        # Initialize prior with zeros\n",
    "        prior = np.zeros(self.num_users, dtype=np.float32)\n",
    "        \n",
    "        # Set priority 1 for the B users with the smallest deadline\n",
    "        for i in range(min(self.max_noma_users, len(sorted_indices))):\n",
    "            if hol_delays[sorted_indices[i]] > 0:  # Only if there's a packet\n",
    "                prior[sorted_indices[i]] = 1.0\n",
    "                \n",
    "        return prior\n",
    "    \n",
    "    def get_channel_prior(self, channel_info, channel_age):\n",
    "        \"\"\"\n",
    "        Channel quality prior\n",
    "        Returns priority based on channel conditions\n",
    "        \"\"\"\n",
    "        # Extract channel quality and age\n",
    "        channel_gains = channel_info[:self.num_users]\n",
    "        ages = channel_age[:self.num_users]\n",
    "        \n",
    "        # Initialize prior with ones\n",
    "        prior = np.ones(self.num_users, dtype=np.float32)\n",
    "        \n",
    "        # Set to 0 for poor channels or outdated channel information\n",
    "        for i in range(self.num_users):\n",
    "            if channel_gains[i] <= self.eta_threshold and ages[i] <= self.tau_threshold:\n",
    "                prior[i] = 0.0\n",
    "                \n",
    "        return prior\n",
    "    \n",
    "    def combine_priors(self, edf_prior, channel_prior):\n",
    "        \"\"\"Combine EDF and channel priors element-wise\"\"\"\n",
    "        return edf_prior * channel_prior\n",
    "    \n",
    "    def get_policy_with_prior(self, agent_state):\n",
    "        \"\"\"\n",
    "        Apply Bayesian policy using prior knowledge\n",
    "        q(a|A; θπ) ∝ π(a|A; θπ) ⊙ f(a;A)\n",
    "        \"\"\"\n",
    "        # Parse state components\n",
    "        buffer_info = agent_state[:self.num_users * 2]  # Buffer info\n",
    "        timing_info = agent_state[self.num_users * 2:self.num_users * 4]  # τp, τa, τs\n",
    "        channel_info = agent_state[self.num_users * 4:self.num_users * 5]  # η\n",
    "        \n",
    "        # Get policy from neural network\n",
    "        raw_policy = self.actor(np.array([agent_state], dtype=np.float32)).numpy()[0]\n",
    "        \n",
    "        # Get EDF prior\n",
    "        edf_prior = self.get_edf_prior(buffer_info)\n",
    "        \n",
    "        # Get channel prior\n",
    "        channel_prior = self.get_channel_prior(channel_info, timing_info[:self.num_users])\n",
    "        \n",
    "        # Combine priors\n",
    "        combined_prior = self.combine_priors(edf_prior, channel_prior)\n",
    "        \n",
    "        # Apply Bayesian update (element-wise multiplication)\n",
    "        posterior_policy = raw_policy * combined_prior\n",
    "        \n",
    "        # Normalize if needed\n",
    "        if np.sum(posterior_policy) > 0:\n",
    "            posterior_policy = posterior_policy / np.sum(posterior_policy)\n",
    "            \n",
    "        return posterior_policy\n",
    "    \n",
    "    def choose_action(self, agent_state, deterministic=False):\n",
    "        \"\"\"Choose action based on policy with prior knowledge\"\"\"\n",
    "        policy = self.get_policy_with_prior(agent_state)\n",
    "        \n",
    "        if deterministic:\n",
    "            # Select users with highest probability up to max_noma_users\n",
    "            sorted_indices = np.argsort(-policy)\n",
    "            action = np.zeros(self.num_users, dtype=np.float32)\n",
    "            count = 0\n",
    "            for idx in sorted_indices:\n",
    "                if policy[idx] > 0.5 and count < self.max_noma_users:\n",
    "                    action[idx] = 1\n",
    "                    count += 1\n",
    "        else:\n",
    "            # Sample action for each user independently\n",
    "            action = np.zeros(self.num_users, dtype=np.float32)\n",
    "            for i in range(self.num_users):\n",
    "                if random.random() < policy[i]:\n",
    "                    action[i] = 1\n",
    "            \n",
    "            # Ensure we don't exceed max_noma_users\n",
    "            if np.sum(action) > self.max_noma_users:\n",
    "                # If too many users selected, keep only the top max_noma_users\n",
    "                indices = np.argsort(-policy)\n",
    "                action = np.zeros(self.num_users, dtype=np.float32)\n",
    "                for i in range(self.max_noma_users):\n",
    "                    action[indices[i]] = 1\n",
    "                    \n",
    "        return action\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def compute_gae(self, rewards, values, next_values, dones):\n",
    "        \"\"\"Compute Generalized Advantage Estimation\"\"\"\n",
    "        advantages = np.zeros_like(rewards, dtype=np.float32)\n",
    "        last_advantage = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            # If episode is done, use reward as terminal value\n",
    "            if dones[t]:\n",
    "                delta = rewards[t] - values[t]\n",
    "            else:\n",
    "                delta = rewards[t] + self.gamma * next_values[t] - values[t]\n",
    "            \n",
    "            # Recursive update of advantage\n",
    "            advantages[t] = delta + self.gamma * self.gae_lambda * last_advantage * (1 - dones[t])\n",
    "            last_advantage = advantages[t]\n",
    "            \n",
    "        # Returns are advantage + value\n",
    "        returns = advantages + values\n",
    "        \n",
    "        return returns, advantages\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Return current metrics for visualization\"\"\"\n",
    "        return {\n",
    "            \"buffer_size\": len(self.buffer),\n",
    "            \"actor_loss\": self.last_actor_loss,\n",
    "            \"critic_loss\": self.last_critic_loss,\n",
    "            \"entropy\": self.last_entropy,\n",
    "        }\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using PPO\"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return {}\n",
    "        \n",
    "        # Sample batch from buffer\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        actions = np.array(actions, dtype=np.float32)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states, dtype=np.float32)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        \n",
    "        # Get values for states and next states\n",
    "        values = self.critic(states).numpy().flatten()\n",
    "        next_values = self.critic(next_states).numpy().flatten()\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns, advantages = self.compute_gae(rewards, values, next_values, dones)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
    "        \n",
    "        # Get old action probabilities\n",
    "        old_probs = np.zeros((self.batch_size, self.num_users), dtype=np.float32)\n",
    "        for i in range(self.batch_size):\n",
    "            policy = self.get_policy_with_prior(states[i])\n",
    "            old_probs[i] = policy\n",
    "        \n",
    "        # Track losses and metrics\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        entropy_values = []\n",
    "        ratios = []\n",
    "        \n",
    "        # PPO training loop\n",
    "        for _ in range(5):  # Multiple epochs of training\n",
    "            indices = np.arange(self.batch_size)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start in range(0, self.batch_size, 32):\n",
    "                end = start + 32\n",
    "                if end <= self.batch_size:\n",
    "                    idx = indices[start:end]\n",
    "                    \n",
    "                    with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:\n",
    "                        # Get current policy\n",
    "                        current_policy = self.actor(tf.convert_to_tensor(states[idx], dtype=tf.float32))\n",
    "                        \n",
    "                        # Apply prior (simplified implementation for demonstration)\n",
    "                        modified_policy = current_policy  # Placeholder for actual implementation\n",
    "                        \n",
    "                        # Add small epsilon to prevent division by zero\n",
    "                        epsilon = 1e-8\n",
    "                        \n",
    "                        # Create masks for actions\n",
    "                        action_mask = actions[idx]\n",
    "                        inverse_action_mask = 1.0 - action_mask\n",
    "                        \n",
    "                        # Compute policy components\n",
    "                        policy_component1 = action_mask * modified_policy\n",
    "                        policy_component2 = inverse_action_mask * (1.0 - modified_policy)\n",
    "                        new_policy_probs = policy_component1 + policy_component2\n",
    "                        \n",
    "                        # Same for old policy\n",
    "                        old_policy_component1 = action_mask * tf.convert_to_tensor(old_probs[idx], dtype=tf.float32)\n",
    "                        old_policy_component2 = inverse_action_mask * (1.0 - tf.convert_to_tensor(old_probs[idx], dtype=tf.float32))\n",
    "                        old_policy_probs = old_policy_component1 + old_policy_component2\n",
    "                        \n",
    "                        # Compute ratio \n",
    "                        new_policy_prob = tf.reduce_prod(new_policy_probs + epsilon, axis=1)\n",
    "                        old_policy_prob = tf.reduce_prod(old_policy_probs + epsilon, axis=1)\n",
    "                        ratio = new_policy_prob / old_policy_prob\n",
    "                        \n",
    "                        # Compute surrogate losses\n",
    "                        advantages_tensor = tf.convert_to_tensor(advantages[idx], dtype=tf.float32)\n",
    "                        surrogate1 = ratio * advantages_tensor\n",
    "                        surrogate2 = tf.clip_by_value(ratio, 1-self.clip_ratio, 1+self.clip_ratio) * advantages_tensor\n",
    "                        \n",
    "                        # Actor loss\n",
    "                        actor_loss = -tf.reduce_mean(tf.minimum(surrogate1, surrogate2))\n",
    "                        \n",
    "                        # Add entropy bonus\n",
    "                        entropy = -tf.reduce_mean(\n",
    "                            modified_policy * tf.math.log(modified_policy + epsilon) + \n",
    "                            (1 - modified_policy) * tf.math.log(1 - modified_policy + epsilon)\n",
    "                        )\n",
    "                        actor_loss = actor_loss - self.entropy_coef * entropy\n",
    "                        \n",
    "                        # Critic loss\n",
    "                        returns_tensor = tf.convert_to_tensor(returns[idx], dtype=tf.float32)\n",
    "                        value_preds = self.critic(states[idx])\n",
    "                        critic_loss = tf.reduce_mean(tf.square(returns_tensor - value_preds))\n",
    "                        \n",
    "                        # Store metrics\n",
    "                        actor_losses.append(actor_loss.numpy())\n",
    "                        critic_losses.append(critic_loss.numpy())\n",
    "                        entropy_values.append(entropy.numpy())\n",
    "                        ratios.append(tf.reduce_mean(ratio).numpy())\n",
    "                    \n",
    "                    # Compute gradients and apply updates\n",
    "                    actor_grads = actor_tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "                    critic_grads = critic_tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "                    \n",
    "                    self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "                    self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        \n",
    "        # Store metrics for visualization\n",
    "        self.last_actor_loss = np.mean(actor_losses) if actor_losses else 0\n",
    "        self.last_critic_loss = np.mean(critic_losses) if critic_losses else 0\n",
    "        self.last_entropy = np.mean(entropy_values) if entropy_values else 0\n",
    "        self.last_ratio = np.mean(ratios) if ratios else 0\n",
    "        \n",
    "        # Clear buffer after training\n",
    "        self.buffer = []\n",
    "        \n",
    "        # Return metrics for visualization\n",
    "        return {\n",
    "            \"actor_loss\": self.last_actor_loss,\n",
    "            \"critic_loss\": self.last_critic_loss,\n",
    "            \"entropy\": self.last_entropy,\n",
    "            \"ratio\": self.last_ratio\n",
    "        }\n",
    "\n",
    "def preprocess_state(buffer_status, timing_info, channel_info, last_reward):\n",
    "    \"\"\"\n",
    "    Preprocess state information into the format expected by the agent.\n",
    "    \n",
    "    Args:\n",
    "        buffer_status: Buffer information (B)\n",
    "        timing_info: Timing information (τp, τa, τs)\n",
    "        channel_info: Channel gains (η)\n",
    "        last_reward: Last received reward r(t-1)\n",
    "        \n",
    "    Returns:\n",
    "        Concatenated and preprocessed state vector\n",
    "    \"\"\"\n",
    "    # Concatenate all information as described in the paper\n",
    "    return np.concatenate([buffer_status, timing_info, channel_info, [last_reward]], dtype=np.float32)\n",
    "\n",
    "def improved_example_usage():\n",
    "    # Example parameters\n",
    "    num_users = 10\n",
    "    max_buffer_size = 5\n",
    "    num_episodes = 50  # Increased for better visualization\n",
    "    \n",
    "    # Initialize the NOMA-PPO agent\n",
    "    agent = NOMA_PPO(\n",
    "        num_users=num_users,\n",
    "        max_buffer_size=max_buffer_size,\n",
    "        discount_factor=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ppo_epsilon=0.2,\n",
    "        actor_learning_rate=0.0003,\n",
    "        critic_learning_rate=0.001,\n",
    "        entropy_coef=0.01,\n",
    "        batch_size=64,\n",
    "        channel_threshold=0.1,\n",
    "        tau_threshold=5,\n",
    "        max_noma_users=4\n",
    "    )\n",
    "    \n",
    "    # Metrics for plotting\n",
    "    rewards_history = []\n",
    "    avg_rewards_history = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd507a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
