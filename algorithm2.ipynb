{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "import scipy.signal\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a490d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55761660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for error probability calculation\n",
    "def Q_function(x):\n",
    "    \"\"\"Q-function: Q(x) = 1/sqrt(2π) * ∫_x^∞ exp(-t²/2)dt\"\"\"\n",
    "    return 0.5 * (1 - norm.cdf(x))\n",
    "\n",
    "def shannon_capacity(gamma):\n",
    "    \"\"\"Shannon capacity: C(γ) = log₂(1 + γ)\"\"\"\n",
    "    return np.log2(1 + gamma)\n",
    "\n",
    "def channel_dispersion(gamma):\n",
    "    \"\"\"Channel dispersion: V(γ) = γ²*(γ+2)/2*(γ+1)² * (log₂(e))²\"\"\"\n",
    "    log2e_squared = (np.log(2))**(-2)\n",
    "    return (gamma*(gamma+2)/((gamma + 1)**2 * 2)) * log2e_squared\n",
    "\n",
    "\n",
    "def compute_error_probability(gamma, n, L):\n",
    "    \"\"\"Compute error probability using finite block length regime\"\"\"\n",
    "    V = channel_dispersion(gamma)\n",
    "    C = shannon_capacity(gamma)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if gamma <= 0 or C <= L/n:\n",
    "        return 1.0\n",
    "    \n",
    "    argument = np.sqrt(n/V) * (C - L/n)\n",
    "    return Q_function(argument)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e86042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the wireless environment\n",
    "class WirelessEnvironment:\n",
    "    def __init__(self, \n",
    "                 num_devices=10,\n",
    "                 num_antennas=4,\n",
    "                 sic_limit=3,\n",
    "                 block_length=200,\n",
    "                 packet_size=256,  # bits\n",
    "                 noise_power=1e-9,\n",
    "                 traffic_type='deterministic_periodic',\n",
    "                 traffic_period=10,\n",
    "                 traffic_probability=1.0,\n",
    "                 poisson_rate=0.1,\n",
    "                 deadline=5,\n",
    "                 area_side=500,  # m\n",
    "                 max_episode_length=200):\n",
    "        \n",
    "        # Network parameters\n",
    "        self.K = num_devices\n",
    "        self.na = num_antennas\n",
    "        self.B = sic_limit\n",
    "        self.n = block_length\n",
    "        self.L = packet_size\n",
    "        self.noise_power = noise_power\n",
    "        \n",
    "        # Traffic parameters\n",
    "        self.traffic_type = traffic_type\n",
    "        self.Np = traffic_period\n",
    "        self.xi = traffic_probability\n",
    "        self.lambda_poisson = poisson_rate\n",
    "        self.delta = deadline\n",
    "        \n",
    "        # Environment dimensions\n",
    "        self.area_side = area_side\n",
    "        self.max_episode_length = max_episode_length\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.total_packets_generated = 0\n",
    "        self.packets_delivered = 0\n",
    "        self.packets_expired = 0\n",
    "        \n",
    "        # Initialize device positions\n",
    "        self.initialize_devices()\n",
    "        \n",
    "        # Initialize time tracking vectors\n",
    "        self.tau_p = np.ones(self.K) * np.inf  # Time since last poll\n",
    "        self.tau_a = np.ones(self.K) * np.inf  # Time since last active\n",
    "        self.tau_s = np.ones(self.K) * np.inf  # Time since last successful decode\n",
    "        \n",
    "        # Buffer initialization\n",
    "        self.reset()\n",
    "        \n",
    "    def initialize_devices(self):\n",
    "        # Random positions in area\n",
    "        x = np.random.uniform(-self.area_side/2, self.area_side/2, self.K)\n",
    "        y = np.random.uniform(-self.area_side/2, self.area_side/2, self.K)\n",
    "        \n",
    "        # Calculate distances to BS\n",
    "        self.distances = np.sqrt(x**2 + y**2)\n",
    "        \n",
    "        # Initialize channel parameters - large scale fading based on distance\n",
    "        self.g = 1 / (1 + self.distances**3)  # Simple path loss model\n",
    "        \n",
    "        # Initialize fast fading channels (will evolve over time)\n",
    "        self.H = np.random.randn(self.na, self.K) + 1j * np.random.randn(self.na, self.K)\n",
    "        self.H = self.H / np.sqrt(2)  # Normalize for complex channels\n",
    "        \n",
    "        # Initialize received powers\n",
    "        self.update_received_powers()\n",
    "        \n",
    "        # Generate offsets for periodic traffic\n",
    "        self.offsets = np.random.randint(0, self.Np, size=self.K)\n",
    "        \n",
    "    def update_received_powers(self):\n",
    "        # Compute received power for each device based on channel conditions\n",
    "        self.eta = np.zeros(self.K)\n",
    "        for k in range(self.K):\n",
    "            h_k = self.H[:, k]\n",
    "            self.eta[k] = self.g[k] * np.sum(np.abs(h_k)**2)\n",
    "    \n",
    "    def evolve_channels(self):\n",
    "        # Time evolution of fast fading using Gauss-Markov model\n",
    "        # Assuming correlation coefficient of 0.9 for simplicity\n",
    "        rho = 0.9\n",
    "        for k in range(self.K):\n",
    "            self.H[:, k] = rho * self.H[:, k] + np.sqrt(1-rho**2) * (np.random.randn(self.na) + 1j * np.random.randn(self.na)) / np.sqrt(2)\n",
    "        \n",
    "        # Update received powers based on new channel state\n",
    "        self.update_received_powers()\n",
    "    \n",
    "    def generate_traffic(self, t):\n",
    "        # Generate new packets according to traffic model\n",
    "        new_packets = np.zeros(self.K, dtype=int)\n",
    "        \n",
    "        if self.traffic_type == 'deterministic_periodic':\n",
    "            for k in range(self.K):\n",
    "                if t % self.Np == self.offsets[k]:\n",
    "                    new_packets[k] = 1\n",
    "                    self.total_packets_generated += 1\n",
    "        \n",
    "        elif self.traffic_type == 'probabilistic_periodic':\n",
    "            for k in range(self.K):\n",
    "                if t % self.Np == self.offsets[k] and np.random.random() < self.xi:\n",
    "                    new_packets[k] = 1\n",
    "                    self.total_packets_generated += 1\n",
    "        \n",
    "        elif self.traffic_type == 'probabilistic_aperiodic':\n",
    "            for k in range(self.K):\n",
    "                # Poisson process\n",
    "                if np.random.random() < self.lambda_poisson:\n",
    "                    new_packets[k] = 1\n",
    "                    self.total_packets_generated += 1\n",
    "        \n",
    "        return new_packets\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset buffer state\n",
    "        self.buffer = np.zeros((self.K, self.delta), dtype=int)\n",
    "        \n",
    "        # Reset time tracking\n",
    "        self.tau_p = np.ones(self.K) * np.inf\n",
    "        self.tau_a = np.ones(self.K) * np.inf\n",
    "        self.tau_s = np.ones(self.K) * np.inf\n",
    "        \n",
    "        # Reset statistics for this episode\n",
    "        self.packets_delivered = 0\n",
    "        self.packets_expired = 0\n",
    "        self.total_packets_generated = 0\n",
    "        \n",
    "        # Initialize channels\n",
    "        self.initialize_devices()\n",
    "        \n",
    "        # Reset time and generate initial packets\n",
    "        self.t = 0\n",
    "        new_packets = self.generate_traffic(self.t)\n",
    "        for k in range(self.K):\n",
    "            if new_packets[k] > 0:\n",
    "                self.buffer[k, self.delta-1] = new_packets[k]\n",
    "        \n",
    "        # Generate observation\n",
    "        obs = self.get_observation()\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def get_head_of_line_delays(self):\n",
    "        # Find the head-of-line delays for each device\n",
    "        d_h = np.ones(self.K) * np.inf\n",
    "        for k in range(self.K):\n",
    "            for d in range(self.delta):\n",
    "                if self.buffer[k, d] > 0:\n",
    "                    d_h[k] = d + 1  # Add 1 because index 0 is delay 1\n",
    "                    break\n",
    "        return d_h\n",
    "    \n",
    "    def get_observation(self):\n",
    "        # Return the current observation for the agent\n",
    "        # This includes buffer observations, channel observations, and timing information\n",
    "        \n",
    "        # Buffer observation - head of line delays\n",
    "        d_h = self.get_head_of_line_delays()\n",
    "        \n",
    "        # Normalize head of line delays to [0, 1]\n",
    "        buffer_obs = np.where(d_h < np.inf, d_h / self.delta, 0)\n",
    "        \n",
    "        # Channel observation - only for active users\n",
    "        u = np.zeros(self.K, dtype=int)\n",
    "        for k in range(self.K):\n",
    "            if np.any(self.buffer[k] > 0):\n",
    "                u[k] = 1\n",
    "        \n",
    "        eta_obs = self.eta * u\n",
    "        \n",
    "        # Timing information\n",
    "        tau_p_norm = 1.0 / np.where(self.tau_p > 0, self.tau_p, np.inf)\n",
    "        tau_a_norm = 1.0 / np.where(self.tau_a > 0, self.tau_a, np.inf)\n",
    "        tau_s_norm = 1.0 / np.where(self.tau_s > 0, self.tau_s, np.inf)\n",
    "        \n",
    "        # Replace inf with 0\n",
    "        tau_p_norm = np.nan_to_num(tau_p_norm)\n",
    "        tau_a_norm = np.nan_to_num(tau_a_norm)\n",
    "        tau_s_norm = np.nan_to_num(tau_s_norm)\n",
    "        \n",
    "        # Create final state vector\n",
    "        obs = np.concatenate([\n",
    "            buffer_obs,\n",
    "            tau_p_norm,\n",
    "            tau_a_norm,\n",
    "            tau_s_norm,\n",
    "            eta_obs\n",
    "        ])\n",
    "        \n",
    "        return obs\n",
    "    def update_buffer_state(self, u, phi):\n",
    "        # Update buffer state based on successful transmissions and new arrivals\n",
    "        \n",
    "        # Remove successfully transmitted packets\n",
    "        for k in range(self.K):\n",
    "            if u[k] == 1 and phi[k] == 1:\n",
    "                # Find the head-of-line packet and remove it\n",
    "                for d in range(self.delta):\n",
    "                    if self.buffer[k, d] > 0:\n",
    "                        self.buffer[k, d] -= 1\n",
    "                        self.packets_delivered += 1\n",
    "                        break\n",
    "        \n",
    "        # Count expired packets\n",
    "        expired = 0\n",
    "        for k in range(self.K):\n",
    "            expired += self.buffer[k, 0]\n",
    "        self.packets_expired += expired\n",
    "        \n",
    "        # Shift packets closer to deadline (from higher index to lower)\n",
    "        new_buffer = np.zeros((self.K, self.delta), dtype=int)\n",
    "        for k in range(self.K):\n",
    "            for d in range(1, self.delta):\n",
    "                new_buffer[k, d-1] = self.buffer[k, d]\n",
    "        self.buffer = new_buffer\n",
    "        \n",
    "        # Generate new traffic\n",
    "        new_packets = self.generate_traffic(self.t)\n",
    "        \n",
    "        # Add new packets to buffer with max deadline\n",
    "        for k in range(self.K):\n",
    "            self.buffer[k, self.delta-1] += new_packets[k]\n",
    "        \n",
    "        return new_packets\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Take action and advance environment\n",
    "        # action is a binary vector of size K indicating which devices to poll\n",
    "        \n",
    "        # Increment time variables\n",
    "        self.tau_p += 1\n",
    "        self.tau_a += 1\n",
    "        self.tau_s += 1\n",
    "        \n",
    "        # Update device timers for polled devices\n",
    "        self.tau_p[action == 1] = 0\n",
    "        \n",
    "        # Determine active devices (polled and have packets to send)\n",
    "        u = np.zeros(self.K, dtype=int)\n",
    "        for k in range(self.K):\n",
    "            if action[k] == 1 and np.any(self.buffer[k] > 0):\n",
    "                u[k] = 1\n",
    "        \n",
    "        # Update time since active\n",
    "        self.tau_a[u == 1] = 0\n",
    "        \n",
    "        # Execute SIC decoding procedure\n",
    "        active_users = np.where(u == 1)[0].tolist()\n",
    "        phi = self.SIC_decoding_procedure(active_users)\n",
    "        \n",
    "        # Update time since successful transmission\n",
    "        self.tau_s[phi == 1] = 0\n",
    "        \n",
    "        # Calculate reward (number of successfully decoded packets)\n",
    "        reward = np.sum(phi)\n",
    "        \n",
    "        # Update buffer state\n",
    "        new_packets = self.update_buffer_state(u, phi)\n",
    "        \n",
    "        # Evolve channels\n",
    "        self.evolve_channels()\n",
    "        \n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "        \n",
    "        # Generate new observation\n",
    "        obs = self.get_observation()\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = (self.t >= self.max_episode_length)\n",
    "        \n",
    "        # Calculate URLLC score (if episode is done)\n",
    "        info = {\n",
    "            \"u\": u, \n",
    "            \"phi\": phi, \n",
    "            \"new_packets\": new_packets,\n",
    "            \"urllc_score\": self.packets_delivered / max(1, self.total_packets_generated) if done else None\n",
    "        }\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def SIC_decoding_procedure(self, active_users):\n",
    "        \"\"\"\n",
    "        SIC Decoding Procedure (Algorithm 1)\n",
    "        \"\"\"\n",
    "        phi = np.zeros(self.K, dtype=int)  # Initialize φₖ(t) = 0, ∀k\n",
    "        \n",
    "        # Check if number of active users exceeds SIC limitation\n",
    "        if len(active_users) <= self.B:\n",
    "            # Create interference matrix between devices\n",
    "            interference_matrix = np.zeros((self.K, self.K))\n",
    "            for j in range(self.K):\n",
    "                for k in range(self.K):\n",
    "                    if j != k:\n",
    "                        # Simple interference model: proportional to power and similarity of channels\n",
    "                        h_j = self.H[:, j]\n",
    "                        h_k = self.H[:, k]\n",
    "                        channel_correlation = np.abs(np.vdot(h_j, h_k))**2 / (np.sum(np.abs(h_j)**2) * np.sum(np.abs(h_k)**2))\n",
    "                        interference_matrix[j, k] = self.eta[j] * channel_correlation * 0.1\n",
    "            \n",
    "            # Sort active users in decreasing order of received power\n",
    "            sorted_indices = np.argsort(-self.eta[active_users])\n",
    "            sorted_active_users = [active_users[i] for i in sorted_indices]\n",
    "            \n",
    "            # Process each active user in decreasing order of received power\n",
    "            for k in sorted_active_users:\n",
    "                # Compute SINR with SIC\n",
    "                gamma_k = self.compute_SINR_with_SIC(k, sorted_active_users, interference_matrix, phi)\n",
    "                \n",
    "                # Compute error probability\n",
    "                epsilon_k = compute_error_probability(gamma_k, self.n, self.L)\n",
    "                \n",
    "                # Draw φₖ(t) from Bernoulli distribution B(1-ϵₖ(t))\n",
    "                phi[k] = np.random.binomial(1, 1-epsilon_k)\n",
    "                \n",
    "        return phi\n",
    "    \n",
    "    def compute_SINR_with_SIC(self, k, sorted_active_users, interference_matrix, phi):\n",
    "        \"\"\"\n",
    "        Compute SINR for device k with SIC\n",
    "        \"\"\"\n",
    "        # Find k's position in the decoding order\n",
    "        k_order = sorted_active_users.index(k)\n",
    "        \n",
    "        # Interference from devices before k in decoding order that were not successfully decoded\n",
    "        before_k = [j for j in sorted_active_users[:k_order] if not phi[j]]\n",
    "        \n",
    "        # Interference from devices after k in decoding order\n",
    "        after_k = sorted_active_users[k_order+1:]\n",
    "        \n",
    "        # Calculate interference\n",
    "        interference = 0\n",
    "        for j in before_k:\n",
    "            interference += interference_matrix[j, k]\n",
    "        \n",
    "        for j in after_k:\n",
    "            interference += interference_matrix[j, k]\n",
    "        \n",
    "        # Calculate SINR\n",
    "        sinr = self.eta[k] / (interference + self.noise_power)\n",
    "        return sinr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDF Prior for the agent\n",
    "def EDF_prior(buffer_obs, K, B):\n",
    "    \"\"\"\n",
    "    Earliest Deadline First prior\n",
    "    Returns a binary vector indicating which devices to schedule based on EDF policy\n",
    "    \"\"\"\n",
    "    # Find devices with packets (non-zero buffer observation)\n",
    "    devices_with_packets = np.where(buffer_obs > 0)[0]\n",
    "    \n",
    "    # If there are fewer devices with packets than SIC limit, schedule all of them\n",
    "    if len(devices_with_packets) <= B:\n",
    "        action = np.zeros(K)\n",
    "        action[devices_with_packets] = 1\n",
    "        return action\n",
    "    \n",
    "    # Otherwise, schedule the B devices with the earliest deadlines\n",
    "    deadlines = buffer_obs[devices_with_packets]\n",
    "    earliest_indices = np.argsort(deadlines)[:B]\n",
    "    to_schedule = devices_with_packets[earliest_indices]\n",
    "    \n",
    "    action = np.zeros(K)\n",
    "    action[to_schedule] = 1\n",
    "    return action\n",
    "\n",
    "# Channel prior\n",
    "def channel_prior(eta, tau_a, K, eta_threshold=1e-8, tau_threshold=5):\n",
    "    \"\"\"\n",
    "    Channel prior: don't schedule devices with poor channel conditions\n",
    "    Returns a binary vector: 0 means poor channel, 1 means good channel\n",
    "    \"\"\"\n",
    "    prior = np.ones(K)\n",
    "    for k in range(K):\n",
    "        if eta[k] <= eta_threshold and tau_a[k] <= tau_threshold:\n",
    "            prior[k] = 0\n",
    "    return prior\n",
    "\n",
    "# Combined prior\n",
    "def bayesian_prior(agent_state, K, B, eta_threshold=1e-8, tau_threshold=5):\n",
    "    \"\"\"\n",
    "    Combine EDF and channel priors into a Bayesian prior\n",
    "    \"\"\"\n",
    "    # Extract components from agent state\n",
    "    buffer_obs = agent_state[:K]\n",
    "    tau_p = agent_state[K:2*K]\n",
    "    tau_a = agent_state[2*K:3*K]\n",
    "    tau_s = agent_state[3*K:4*K]\n",
    "    eta = agent_state[4*K:]\n",
    "    \n",
    "    # Get individual priors\n",
    "    edf_prior = EDF_prior(buffer_obs, K, B)\n",
    "    ch_prior = channel_prior(eta, tau_a, K, eta_threshold, tau_threshold)\n",
    "    \n",
    "    # Combine priors (element-wise multiplication)\n",
    "    combined_prior = edf_prior * ch_prior\n",
    "    \n",
    "    return combined_prior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy and value networks with branching architecture\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Individual branch for each device\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(output_dim)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Process through shared layers\n",
    "        shared_features = self.shared_layers(x)\n",
    "        \n",
    "        # Process through individual branches\n",
    "        device_probs = torch.zeros(x.size(0), len(self.branches), device=x.device)\n",
    "        for i, branch in enumerate(self.branches):\n",
    "            device_probs[:, i] = branch(shared_features).squeeze(-1)\n",
    "        \n",
    "        return device_probs\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# Experience buffer for PPO\n",
    "class PPOBuffer:\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        assert self.ptr < self.max_size\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = self.discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # Rewards-to-go, which is target value for value network\n",
    "        self.ret_buf[path_slice] = self.discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def discount_cumsum(self, x, discount):\n",
    "        \"\"\"\n",
    "        Compute discounted cumulative sums of vectors.\n",
    "        \"\"\"\n",
    "        return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "    def get(self):\n",
    "        assert self.ptr == self.max_size    # Buffer must be full before using\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # Advantage normalization\n",
    "        adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / (adv_std + 1e-8)\n",
    "        \n",
    "        return [\n",
    "            self.obs_buf, self.act_buf, self.ret_buf, \n",
    "            self.adv_buf, self.logp_buf\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOMA-PPO Agent calling\n",
    "class NOMA_PPO:\n",
    "    def __init__(\n",
    "        self, \n",
    "        env,\n",
    "        hidden_dim=256,\n",
    "        gamma=0.99,\n",
    "        clip_ratio=0.2,\n",
    "        pi_lr=3e-4,\n",
    "        vf_lr=1e-3,\n",
    "        train_pi_iters=80,\n",
    "        train_v_iters=80,\n",
    "        lam=0.97,\n",
    "        target_kl=0.01,\n",
    "        eta_threshold=1e-8,\n",
    "        tau_threshold=5\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.train_pi_iters = train_pi_iters\n",
    "        self.train_v_iters = train_v_iters\n",
    "        self.target_kl = target_kl\n",
    "        self.eta_threshold = eta_threshold\n",
    "        self.tau_threshold = tau_threshold\n",
    "        \n",
    "        # Get environment dimensions\n",
    "        obs = env.reset()\n",
    "        self.obs_dim = len(obs)\n",
    "        self.act_dim = env.K\n",
    "        \n",
    "        # Initialize policy and value networks\n",
    "        self.policy = PolicyNetwork(self.obs_dim, self.act_dim, hidden_dim).to(device)\n",
    "        self.value_net = ValueNetwork(self.obs_dim, hidden_dim).to(device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.pi_optimizer = optim.Adam(self.policy.parameters(), lr=pi_lr)\n",
    "        self.vf_optimizer = optim.Adam(self.value_net.parameters(), lr=vf_lr)\n",
    "        \n",
    "        # Action selection\n",
    "        self.use_prior = True\n",
    "    \n",
    "    def get_action(self, obs, deterministic=False):\n",
    "        # Convert observation to tensor\n",
    "        obs_tensor = torch.as_tensor(obs, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Get policy output (probabilities for each device)\n",
    "        with torch.no_grad():\n",
    "            policy_output = self.policy(obs_tensor.unsqueeze(0)).squeeze(0)\n",
    "        \n",
    "        # Apply prior if enabled\n",
    "        if self.use_prior:\n",
    "            prior = bayesian_prior(obs, self.act_dim, self.env.B, \n",
    "                                 self.eta_threshold, self.tau_threshold)\n",
    "            \n",
    "            # Convert prior to tensor\n",
    "            prior_tensor = torch.as_tensor(prior, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Combine policy output with prior (element-wise multiplication)\n",
    "            combined_probs = policy_output * prior_tensor\n",
    "            \n",
    "            # Normalize (if needed)\n",
    "            combined_probs = torch.clamp(combined_probs, min=1e-8, max=1-1e-8)\n",
    "        else:\n",
    "            combined_probs = policy_output\n",
    "        \n",
    "        # Sample action for each device\n",
    "        if deterministic:\n",
    "            action = (combined_probs > 0.5).float()\n",
    "        else:\n",
    "            dist = Bernoulli(probs=combined_probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "        # Calculate log probabilities\n",
    "        log_probs = torch.log(combined_probs * action + (1 - combined_probs) * (1 - action) + 1e-8)\n",
    "        \n",
    "        return action.cpu().numpy(), log_probs.cpu().numpy()\n",
    "    \n",
    "    def compute_value(self, obs):\n",
    "        obs_tensor = torch.as_tensor(obs, dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            value = self.value_net(obs_tensor.unsqueeze(0)).squeeze()\n",
    "        return value.cpu().numpy()\n",
    "    \n",
    "    def update(self, obs_buf, act_buf, ret_buf, adv_buf, logp_buf):\n",
    "        # Convert buffers to tensors\n",
    "        obs_buf = torch.as_tensor(obs_buf, dtype=torch.float32).to(device)\n",
    "        act_buf = torch.as_tensor(act_buf, dtype=torch.float32).to(device)\n",
    "        ret_buf = torch.as_tensor(ret_buf, dtype=torch.float32).to(device)\n",
    "        adv_buf = torch.as_tensor(adv_buf, dtype=torch.float32).to(device)\n",
    "        logp_old_buf = torch.as_tensor(logp_buf, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Train policy with multiple steps of gradient descent\n",
    "        for i in range(self.train_pi_iters):\n",
    "            # Get policy output\n",
    "            policy_output = self.policy(obs_buf)\n",
    "            \n",
    "            # Apply prior if enabled (for each observation in batch)\n",
    "            if self.use_prior:\n",
    "                batch_priors = []\n",
    "                for obs_idx in range(len(obs_buf)):\n",
    "                    prior = bayesian_prior(obs_buf[obs_idx].cpu().numpy(), \n",
    "                                         self.act_dim, self.env.B,\n",
    "                                         self.eta_threshold, self.tau_threshold)\n",
    "                    batch_priors.append(prior)\n",
    "                \n",
    "                # Convert to tensor and apply\n",
    "                prior_tensor = torch.as_tensor(batch_priors, dtype=torch.float32).to(device)\n",
    "                combined_probs = policy_output * prior_tensor\n",
    "                combined_probs = torch.clamp(combined_probs, min=1e-8, max=1-1e-8)\n",
    "            else:\n",
    "                combined_probs = policy_output\n",
    "            \n",
    "            # Calculate log probabilities of actions\n",
    "            log_probs = torch.log(combined_probs * act_buf + (1 - combined_probs) * (1 - act_buf) + 1e-8)\n",
    "            \n",
    "            # Calculate ratio between new and old policies\n",
    "            ratio = torch.exp(log_probs - logp_old_buf)\n",
    "            \n",
    "            # Calculate surrogate losses\n",
    "            clip_adv = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * adv_buf.unsqueeze(-1)\n",
    "            surr1 = ratio * adv_buf.unsqueeze(-1)\n",
    "            surr2 = clip_adv\n",
    "            \n",
    "            # PPO loss\n",
    "            pi_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Calculate approximate KL divergence for early stopping\n",
    "            kl = (logp_old_buf - log_probs).mean().item()\n",
    "            \n",
    "            # If KL divergence exceeds threshold, break\n",
    "            if kl > 1.5 * self.target_kl:\n",
    "                # Early stopping at iteration {i} due to reaching max KL divergence.\n",
    "                print(f\"sai\")\n",
    "                break\n",
    "            \n",
    "            # Perform gradient step\n",
    "            self.pi_optimizer.zero_grad()\n",
    "            pi_loss.backward()\n",
    "            self.pi_optimizer.step()\n",
    "        \n",
    "        # Value function learning\n",
    "        for i in range(self.train_v_iters):\n",
    "            # Predict values\n",
    "            values = self.value_net(obs_buf).squeeze()\n",
    "            \n",
    "            # Calculate value loss (MSE)\n",
    "            v_loss = ((values - ret_buf) ** 2).mean()\n",
    "            \n",
    "            # Perform gradient step\n",
    "            self.vf_optimizer.zero_grad()\n",
    "            v_loss.backward()\n",
    "            self.vf_optimizer.step()\n",
    "    \n",
    "    def train(self, epochs=50, steps_per_epoch=4000, max_ep_len=1000, batch_size=64):\n",
    "        \"\"\"\n",
    "        Implementation of NOMA-PPO (Algorithm 2)\n",
    "        \"\"\"\n",
    "        # Initialize storage\n",
    "        total_steps = steps_per_epoch * epochs\n",
    "        obs_buf = np.zeros((total_steps, self.obs_dim), dtype=np.float32)\n",
    "        act_buf = np.zeros((total_steps, self.act_dim), dtype=np.float32)\n",
    "        rew_buf = np.zeros(total_steps, dtype=np.float32)\n",
    "        val_buf = np.zeros(total_steps, dtype=np.float32)\n",
    "        logp_buf = np.zeros((total_steps, self.act_dim), dtype=np.float32)\n",
    "        \n",
    "        # For tracking progress\n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        urllc_scores = []\n",
    "        \n",
    "        # Initialize environment\n",
    "        obs = self.env.reset()\n",
    "        ep_reward = 0\n",
    "        ep_length = 0\n",
    "        \n",
    "        # Main training loop\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            for t in range(steps_per_epoch):\n",
    "                step_idx = epoch * steps_per_epoch + t\n",
    "                \n",
    "                # Get action and value\n",
    "                act, logp = self.get_action(obs)\n",
    "                val = self.compute_value(obs)\n",
    "                \n",
    "                # Store data\n",
    "                obs_buf[step_idx] = obs\n",
    "                act_buf[step_idx] = act\n",
    "                logp_buf[step_idx] = logp\n",
    "                val_buf[step_idx] = val\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_obs, rew, done, info = self.env.step(act)\n",
    "                \n",
    "                # Store reward\n",
    "                rew_buf[step_idx] = rew\n",
    "                \n",
    "                # Update episode stats\n",
    "                ep_reward += rew\n",
    "                ep_length += 1\n",
    "                \n",
    "                # Move to next observation\n",
    "                obs = next_obs\n",
    "                \n",
    "                # End of episode handling\n",
    "                if done or (ep_length == max_ep_len):\n",
    "                    # Store episode results\n",
    "                    episode_rewards.append(ep_reward)\n",
    "                    episode_lengths.append(ep_length)\n",
    "                    if info[\"urllc_score\"] is not None:\n",
    "                        urllc_scores.append(info[\"urllc_score\"])\n",
    "                    \n",
    "                    # Reset env for next episode\n",
    "                    obs = self.env.reset()\n",
    "                    ep_reward = 0\n",
    "                    ep_length = 0\n",
    "            \n",
    "            # Update policy and value network (after each epoch)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            # Compute returns and advantages\n",
    "            last_val = 0 if done else self.compute_value(obs)\n",
    "            returns = self.compute_returns(rew_buf[epoch*steps_per_epoch:(epoch+1)*steps_per_epoch], \n",
    "                                          val_buf[epoch*steps_per_epoch:(epoch+1)*steps_per_epoch], \n",
    "                                          last_val)\n",
    "            \n",
    "            advs = returns - val_buf[epoch*steps_per_epoch:(epoch+1)*steps_per_epoch]\n",
    "            advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "            \n",
    "            # Update using mini-batches\n",
    "            indices = np.arange(steps_per_epoch)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start in range(0, steps_per_epoch, batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                # Get batch data\n",
    "                obs_batch = obs_buf[epoch*steps_per_epoch + batch_indices]\n",
    "                act_batch = act_buf[epoch*steps_per_epoch + batch_indices]\n",
    "                ret_batch = returns[batch_indices]\n",
    "                adv_batch = advs[batch_indices]\n",
    "                logp_batch = logp_buf[epoch*steps_per_epoch + batch_indices]\n",
    "                \n",
    "                # Update policy and value networks\n",
    "                self.update(obs_batch, act_batch, ret_batch, adv_batch, logp_batch)\n",
    "            \n",
    "           # Print progress\n",
    "            if len(urllc_scores) > 0:\n",
    "                print(f\"URLLC Score: {urllc_scores[-1]:.4f}\")\n",
    "                \n",
    "         # Training complete\n",
    "        print(f\"Training complete after {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Return training statistics for plotting\n",
    "        return {\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'episode_lengths': episode_lengths,\n",
    "            'urllc_scores': urllc_scores\n",
    "        }\n",
    "            \n",
    "    def compute_returns(self, rewards, values, last_val):\n",
    "        \"\"\"\n",
    "        Compute returns using GAE\n",
    "        \"\"\"\n",
    "        path_slice = len(rewards)\n",
    "        rews = np.append(rewards, last_val)\n",
    "        vals = np.append(values, last_val)\n",
    "        \n",
    "        # GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        \n",
    "        # Compute GAE advantages\n",
    "        advantages = np.zeros_like(deltas)\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            gae = deltas[t] + self.gamma * 0.95 * gae\n",
    "            advantages[t] = gae\n",
    "        \n",
    "        # Returns = advantages + values\n",
    "        returns = advantages + values\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def save(self, path=\"noma_ppo_model\"):\n",
    "        \"\"\"Save model to file\"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        torch.save({\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'value_state_dict': self.value_net.state_dict(),\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load(self, path=\"noma_ppo_model\"):\n",
    "        \"\"\"Load model from file\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.value_net.load_state_dict(checkpoint['value_state_dict'])\n",
    "        print(f\"Model loaded from {path}\")\n",
    "    \n",
    "def evaluate(self, num_episodes=10):\n",
    "        \"\"\"Evaluate the agent's performance\"\"\"\n",
    "        ep_rewards = []\n",
    "        ep_lengths = []\n",
    "        urllc_scores = []\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            ep_reward = 0\n",
    "            ep_length = 0\n",
    "            \n",
    "            while not done:\n",
    "                # Get action deterministically (using policy mean)\n",
    "                action, _ = self.get_action(obs, deterministic=True)\n",
    "                \n",
    "                # Take step in environment\n",
    "                obs, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                ep_reward += reward\n",
    "                ep_length += 1\n",
    "            \n",
    "            # Record episode statistics\n",
    "            ep_rewards.append(ep_reward)\n",
    "            ep_lengths.append(ep_length)\n",
    "            urllc_scores.append(info[\"urllc_score\"])\n",
    "        \n",
    "        # Return average statistics\n",
    "        return {\n",
    "            \"avg_reward\": np.mean(ep_rewards),\n",
    "            \"avg_length\": np.mean(ep_lengths),\n",
    "            \"avg_urllc_score\": np.mean(urllc_scores),\n",
    "            \"urllc_scores\": urllc_scores\n",
    "        }\n",
    "\n",
    "\n",
    "# Implement Random scheduler (baseline)\n",
    "def random_scheduler(env, num_episodes=10):\n",
    "    \"\"\"Random scheduler that selects B devices randomly each time\"\"\"\n",
    "    urllc_scores = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Randomly select B devices\n",
    "            action = np.zeros(env.K)\n",
    "            indices = np.random.choice(env.K, size=env.B, replace=False)\n",
    "            action[indices] = 1\n",
    "            \n",
    "            # Take step in environment\n",
    "            obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        urllc_scores.append(info[\"urllc_score\"])\n",
    "    \n",
    "    return np.mean(urllc_scores)\n",
    "\n",
    "\n",
    "# Implement EDF scheduler (baseline)\n",
    "def edf_scheduler(env, num_episodes=10):\n",
    "    \"\"\"EDF scheduler that selects devices with earliest deadlines\"\"\"\n",
    "    urllc_scores = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Extract buffer observations from state\n",
    "            buffer_obs = obs[:env.K]\n",
    "            \n",
    "            # Get EDF action\n",
    "            action = EDF_prior(buffer_obs, env.K, env.B)\n",
    "            \n",
    "            # Take step in environment\n",
    "            obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        urllc_scores.append(info[\"urllc_score\"])\n",
    "    \n",
    "    return np.mean(urllc_scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4c0bff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training on Deterministic Periodic ---\n",
      "Epoch 1/10\n",
      "Early stopping at iteration 0 due to reaching max KL divergence.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m agent \u001b[38;5;241m=\u001b[39m NOMA_PPO(env)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m train_stats \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ep_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Evaluate agent\u001b[39;00m\n\u001b[0;32m     31\u001b[0m eval_stats \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mevaluate(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 246\u001b[0m, in \u001b[0;36mNOMA_PPO.train\u001b[1;34m(self, epochs, steps_per_epoch, max_ep_len, batch_size)\u001b[0m\n\u001b[0;32m    243\u001b[0m      logp_batch \u001b[38;5;241m=\u001b[39m logp_buf[epoch\u001b[38;5;241m*\u001b[39msteps_per_epoch \u001b[38;5;241m+\u001b[39m batch_indices]\n\u001b[0;32m    245\u001b[0m      \u001b[38;5;66;03m# Update policy and value networks\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m      \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogp_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Print progress\u001b[39;00m\n\u001b[0;32m    249\u001b[0m  \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(urllc_scores) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[17], line 96\u001b[0m, in \u001b[0;36mNOMA_PPO.update\u001b[1;34m(self, obs_buf, act_buf, ret_buf, adv_buf, logp_buf)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Train policy with multiple steps of gradient descent\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pi_iters):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Get policy output\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     policy_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_buf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# Apply prior if enabled (for each observation in batch)\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_prior:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m device_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranches), device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, branch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranches):\n\u001b[1;32m---> 31\u001b[0m     device_probs[:, i] \u001b[38;5;241m=\u001b[39m \u001b[43mbranch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_features\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device_probs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Usage example with plots\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environments with different configurations\n",
    "    env_configs = [\n",
    "        {\"name\": \"Deterministic Periodic\", \"traffic_type\": \"deterministic_periodic\", \"num_devices\": 10},\n",
    "        {\"name\": \"Probabilistic Aperiodic\", \"traffic_type\": \"probabilistic_aperiodic\", \"num_devices\": 10},\n",
    "        {\"name\": \"High Device Density\", \"traffic_type\": \"deterministic_periodic\", \"num_devices\": 20},\n",
    "    ]\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Train and evaluate NOMA-PPO on each environment configuration\n",
    "    for config in env_configs:\n",
    "        print(f\"\\n--- Training on {config['name']} ---\")\n",
    "        \n",
    "        # Create environment\n",
    "        env = WirelessEnvironment(\n",
    "            num_devices=config[\"num_devices\"],\n",
    "            traffic_type=config[\"traffic_type\"],\n",
    "            max_episode_length=100  # Shorter for demonstration\n",
    "        )\n",
    "        \n",
    "        # Create agent\n",
    "        agent = NOMA_PPO(env)\n",
    "        \n",
    "        # Train agent\n",
    "        train_stats = agent.train(epochs=10, steps_per_epoch=1000, max_ep_len=100, batch_size=64)\n",
    "        \n",
    "        # Evaluate agent\n",
    "        eval_stats = agent.evaluate(num_episodes=5)\n",
    "        \n",
    "        # Evaluate baselines\n",
    "        random_score = random_scheduler(env, num_episodes=5)\n",
    "        edf_score = edf_scheduler(env, num_episodes=5)\n",
    "        \n",
    "        # Store results\n",
    "        results[config[\"name\"]] = {\n",
    "            \"train_stats\": train_stats,\n",
    "            \"eval_stats\": eval_stats,\n",
    "            \"random_score\": random_score,\n",
    "            \"edf_score\": edf_score\n",
    "        }\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for config_name, data in results.items():\n",
    "        plt.plot(data[\"train_stats\"][\"urllc_scores\"], label=config_name)\n",
    "    plt.title(\"URLLC Score During Training\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"URLLC Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot episode rewards\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for config_name, data in results.items():\n",
    "        plt.plot(data[\"train_stats\"][\"episode_rewards\"], label=config_name)\n",
    "    plt.title(\"Episode Rewards During Training\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"noma_ppo_training.png\")\n",
    "    \n",
    "    # Plot algorithm comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Prepare data for bar chart\n",
    "    config_names = list(results.keys())\n",
    "    noma_ppo_scores = [data[\"eval_stats\"][\"avg_urllc_score\"] for data in results.values()]\n",
    "    random_scores = [data[\"random_score\"] for data in results.values()]\n",
    "    edf_scores = [data[\"edf_score\"] for data in results.values()]\n",
    "    \n",
    "    # Set width of bars\n",
    "    bar_width = 0.25\n",
    "    indices = np.arange(len(config_names))\n",
    "    \n",
    "    # Create bars\n",
    "    plt.bar(indices - bar_width, noma_ppo_scores, bar_width, label='NOMA-PPO')\n",
    "    plt.bar(indices, random_scores, bar_width, label='Random')\n",
    "    plt.bar(indices + bar_width, edf_scores, bar_width, label='EDF')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Environment Configuration')\n",
    "    plt.ylabel('URLLC Score')\n",
    "    plt.title('Algorithm Comparison')\n",
    "    plt.xticks(indices, config_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"algorithm_comparison.png\")\n",
    "    \n",
    "    # Plot device-level statistics for one configuration\n",
    "    config_name = \"Deterministic Periodic\"\n",
    "    env = WirelessEnvironment(\n",
    "        num_devices=10,\n",
    "        traffic_type=\"deterministic_periodic\",\n",
    "        max_episode_length=200\n",
    "    )\n",
    "    \n",
    "    # Run agent for one episode to collect device statistics\n",
    "    agent = NOMA_PPO(env)\n",
    "    agent.load(\"noma_ppo_model\")  # If saved during previous training\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    device_polls = np.zeros(env.K)\n",
    "    device_active = np.zeros(env.K)\n",
    "    device_success = np.zeros(env.K)\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = agent.get_action(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update device statistics\n",
    "        device_polls += action\n",
    "        device_active += info[\"u\"]\n",
    "        device_success += info[\"phi\"]\n",
    "    \n",
    "    # Plot device-level statistics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(env.K)\n",
    "    width = 0.3\n",
    "    \n",
    "    plt.bar(x - width, device_polls, width, label='Times Polled')\n",
    "    plt.bar(x, device_active, width, label='Times Active')\n",
    "    plt.bar(x + width, device_success, width, label='Successful Transmissions')\n",
    "    \n",
    "    plt.xlabel('Device ID')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Device-Level Statistics')\n",
    "    plt.xticks(x)\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"device_statistics.png\")\n",
    "    \n",
    "    print(\"\\n--- Summary of Results ---\")\n",
    "    for config_name, data in results.items():\n",
    "        print(f\"\\nConfiguration: {config_name}\")\n",
    "        print(f\"NOMA-PPO URLLC Score: {data['eval_stats']['avg_urllc_score']:.4f}\")\n",
    "        print(f\"Random Scheduler URLLC Score: {data['random_score']:.4f}\")\n",
    "        print(f\"EDF Scheduler URLLC Score: {data['edf_score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
